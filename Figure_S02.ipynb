{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zhang Chao, 2025.04.07<br>\n",
    "Plotting the mean irrigation cooling effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import regionmask\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "from matplotlib.colors import Normalize\n",
    "from cartopy.feature import ShapelyFeature\n",
    "import matplotlib.colors as mcolors\n",
    "sys.path.append('/home/climate/chaoz/code/utils/')\n",
    "from plot_utils import plot_settings, uneven_cmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/climate/chaoz/project/03Irr_Ts_CN/processed/')\n",
    "\n",
    "dLSTday   = xr.open_dataset('delta_LSTday_Yr_CN_2001_2020.nc')\n",
    "dLSTnight = xr.open_dataset('delta_LSTnight_Yr_CN_2001_2020.nc')\n",
    "\n",
    "# Below two input files were produced by './2_process/06_resampleto05d.ipynb'\n",
    "dLSTday_Yr_05d = xr.open_dataset('delta_LSTday_Yr_CN_2001_2020_05d.nc')\n",
    "dLSTnight_Yr_05d = xr.open_dataset('delta_LSTnight_Yr_CN_2001_2020_05d.nc')\n",
    "\n",
    "dLSTday_mean = xr.open_dataset('delta_LSTday_Yr_CN_Mean.nc')\n",
    "dLSTnight_mean =xr.open_dataset('delta_LSTnight_Yr_CN_Mean.nc')\n",
    "dLSTday_Yr_05d_mean=xr.open_dataset('delta_LSTday_Yr_CN_Mean_05d.nc')\n",
    "dLSTnight_Yr_05d_mean=xr.open_dataset('delta_LSTnight_Yr_CN_Mean_05d.nc')\n",
    "dLSTday_Mn_mean=xr.open_dataset('delta_LSTday_Mn_mean_CN_2001_2020.nc')\n",
    "dLSTnight_Mn_mean=xr.open_dataset('delta_LSTnight_Mn_mean_CN_2001_2020.nc')\n",
    "\n",
    "shp_cn = gpd.read_file('../shapefile/ChinaAll.shp')\n",
    "shp_nanhai = gpd.read_file('../shapefile/Nanhai.shp')\n",
    "shp_climzone = gpd.read_file('../shapefile/ClimateZone_3.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_regionmean_95p(ds, varname, shp):\n",
    "    \"\"\"Calculate the national and regional mean values and 95% CI half-width.\n",
    "\n",
    "    The 95% CI half-width is calculated as: std / sqrt(n) * 1.96,\n",
    "    approximating the confidence interval for the mean based on the\n",
    "    standard deviation of the spatial grid cells within each region/nation.\n",
    "\n",
    "    Args:\n",
    "        ds (xarray dataset): time-series dataset containing the variable\n",
    "                             with 'lat' and 'lon' dimensions.\n",
    "        varname (str): the variable name in the xarray dataset.\n",
    "        shp (geo dataframe): shapefile loaded by geopandas.read_file()\n",
    "                             defining the regions.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: time-series means and 95p CI half-widths\n",
    "                          for the nation and different regions.\n",
    "    \"\"\"\n",
    "    # Ensure we are working with the specific variable DataArray\n",
    "    da = ds[varname]\n",
    "\n",
    "    # --- National Calculation ---\n",
    "    # Count non-null values for the specific variable nationally\n",
    "    national_count = da.notnull().sum().item()\n",
    "    # Calculate national mean\n",
    "    cn_mean = da.mean(dim=('lat', 'lon')).item()\n",
    "    # Calculate national standard deviation\n",
    "    cn_std_dev = da.std(dim=('lat', 'lon')).item()\n",
    "    # Calculate national 95p CI half-width\n",
    "    # Handle case where count might be zero\n",
    "    if national_count > 0:\n",
    "        cn_95p = (cn_std_dev / np.sqrt(national_count)) * 1.96\n",
    "    else:\n",
    "        cn_95p = np.nan # Assign NaN if no valid data points\n",
    "\n",
    "    # --- Regional Calculation ---\n",
    "    # Create the region mask\n",
    "    mask_region = regionmask.mask_geopandas(shp, da.lon, da.lat)\n",
    "\n",
    "    # Calculate regional means\n",
    "    region_mean = da.groupby(mask_region).mean()\n",
    "    # Calculate regional standard deviations\n",
    "    region_std_dev = da.groupby(mask_region).std()\n",
    "    # Calculate regional counts (number of non-null cells per region)\n",
    "    region_count = da.notnull().groupby(mask_region).sum()\n",
    "\n",
    "    # Calculate regional 95p CI half-width\n",
    "    # Use xarray's where to handle potential division by zero if count is 0\n",
    "    regional_95p = (region_std_dev / np.sqrt(region_count)).where(region_count > 0) * 1.96\n",
    "\n",
    "    # Convert regional results to DataFrames\n",
    "    region_mean_df = region_mean.to_dataframe(name='mean').reset_index()\n",
    "    # Use the calculated regional_95p directly\n",
    "    region_95p_df = regional_95p.to_dataframe(name='95p').reset_index()\n",
    "\n",
    "    if 'name' in shp.columns: # Example: if shp has a 'name' column\n",
    "         mask_mapping = {i: name for i, name in enumerate(shp['name'])}\n",
    "    else: # Fallback to your original mapping if no name column\n",
    "        mask_mapping = {1.0:'Arid', 2.0:'Semi', 0.0:'Humid'}\n",
    "        print(\"Warning: Using default mask mapping. Verify it matches your shapefile regions.\")\n",
    "\n",
    "\n",
    "    region_mean_df['mask'] = region_mean_df['mask'].replace(mask_mapping)\n",
    "    region_95p_df['mask'] = region_95p_df['mask'].replace(mask_mapping)\n",
    "\n",
    "    # --- Combine Results ---\n",
    "    # Merge regional mean and 95p based on the mask name\n",
    "    df1 = pd.merge(region_mean_df[['mask', 'mean']],\n",
    "                   region_95p_df[['mask', '95p']],\n",
    "                   on='mask')\n",
    "\n",
    "    # Create DataFrame for national results\n",
    "    df2 = pd.DataFrame({'mask': ['China'], # Assuming 'China' is the national label\n",
    "                        'mean': [cn_mean],\n",
    "                        '95p': [cn_95p]}) # Use the new column name '95p'\n",
    "\n",
    "    # Concatenate regional and national results\n",
    "    df3 = pd.concat([df1, df2], axis=0, ignore_index=True).set_index('mask')\n",
    "\n",
    "    # Reindex to desired order (ensure region names match those from mask_mapping)\n",
    "    # Get the region names from the mapping used\n",
    "    region_order = ['China'] + list(mask_mapping.values())\n",
    "    # Filter df3 index to only include expected regions before reindexing\n",
    "    df3 = df3[df3.index.isin(region_order)]\n",
    "    df3 = df3.reindex(region_order) # Use the actual region names\n",
    "\n",
    "    return df3\n",
    "\n",
    "\n",
    "def stats_regionmean(ds,varname,shp):\n",
    "    \"\"\"Calculate the national and regional mean values\n",
    "\n",
    "    Args:\n",
    "        ds (xarray dataset): time-series dataset\n",
    "        varname (str): the variable name in the xarray dataset\n",
    "        shp (geo dataframe): shapefile loaded by geopandas.read_file()\n",
    "\n",
    "    Returns:\n",
    "        numpy.dataframe: time-series annual means for different regions\n",
    "    \"\"\"\n",
    "    # Calculate the national mean values\n",
    "    stat_cn = ds.mean(dim=('lat','lon'))[varname].values\n",
    "    \n",
    "    mask_region = regionmask.mask_geopandas(shp,ds.lon,ds.lat)\n",
    "    stat_region = ds.groupby(mask_region).mean().to_dataframe().reset_index()\n",
    "    \n",
    "    mask_mapping = {0.0:'Humid', 1.0:'Arid', 2.0:'Semi'}\n",
    "    stat_region['mask'] = stat_region['mask'].replace(mask_mapping)\n",
    "    \n",
    "    # Rearrange the dataframe by the different masks (regions)\n",
    "    pivoted_df = stat_region.pivot_table(index='time', columns='mask', values=varname).reset_index()\n",
    "    # Append the national mean values as a column 'China'\n",
    "    pivoted_df['China'] = stat_cn\n",
    "    \n",
    "    # Re-order the columns\n",
    "    pivoted_df = pivoted_df[['time','China','Arid','Semi','Humid']]\n",
    "    \n",
    "    return pivoted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Using default mask mapping. Verify it matches your shapefile regions.\n",
      "Warning: Using default mask mapping. Verify it matches your shapefile regions.\n"
     ]
    }
   ],
   "source": [
    "df_dLSTd_Yrmean = stats_regionmean_95p(dLSTday_mean,'Ts',shp_climzone)\n",
    "df_dLSTn_Yrmean = stats_regionmean_95p(dLSTnight_mean,'Ts',shp_climzone)\n",
    "\n",
    "df_dLSTd_Mnmean = stats_regionmean(dLSTday_Mn_mean,'Ts',shp_climzone)\n",
    "df_dLSTn_Mnmean = stats_regionmean(dLSTnight_Mn_mean,'Ts',shp_climzone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_map(fig, pos, ds, extent,shape_cn,shape_nanhai,levels, mycmap, No,fameon_sing,nanhai_sign):\n",
    "    lambert_proj = ccrs.LambertConformal(central_longitude=105, central_latitude=35, standard_parallels=(25, 47))\n",
    "    ax = fig.add_axes(pos, projection=lambert_proj, frame_on=fameon_sing)\n",
    "\n",
    "    ds.Ts.plot(ax=ax, levels=levels,transform=ccrs.PlateCarree(),#lambert_proj\n",
    "                     cmap=mycmap, add_colorbar=False, rasterized=True) #vmax=vmax, vmin=vmin,\n",
    "    ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "    fea_cn = ShapelyFeature(shape_cn.geometry, crs=ccrs.PlateCarree())\n",
    "    fea_nanhai = ShapelyFeature(shape_nanhai.geometry, crs=ccrs.PlateCarree())\n",
    "    ax.add_feature(fea_cn,facecolor='none',linewidth=.6)\n",
    "    ax.add_feature(fea_nanhai,facecolor='none',linewidth=.6)\n",
    "    \n",
    "    ax.text(0.01, 1.0, No, transform=ax.transAxes, c='k', weight='bold', fontsize=14)\n",
    "    ax.set_title('')\n",
    "    # Add the South China Sea map\n",
    "    if nanhai_sign:\n",
    "        hx,vx = 0.0381,0.10\n",
    "        pos_scs = [ax.get_position().x1-hx,ax.get_position().y0,hx,vx]\n",
    "        ax_scs = fig.add_axes(pos_scs,projection=lambert_proj,frame_on=True)\n",
    "        ax_scs.add_feature(fea_cn    ,facecolor='none',linewidth=.6)\n",
    "        ax_scs.add_feature(fea_nanhai,facecolor='none',linewidth=.6)\n",
    "        ax_scs.set_extent([107, 120,3,23], crs=ccrs.PlateCarree())\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_lat_stats(fig,pos,ds,ylim):\n",
    "    ax = fig.add_axes(pos)\n",
    "    lat_stats = ds.quantile([0.25,0.50,0.75], dim='lon')\n",
    "    lat = lat_stats.lat\n",
    "    p25 = lat_stats.sel(quantile=0.25)\n",
    "    p50 = lat_stats.sel(quantile=0.50)\n",
    "    p75 = lat_stats.sel(quantile=0.75)\n",
    "    \n",
    "    # Fill the region between the 25th and 75th percentiles\n",
    "    ax.fill_betweenx(lat,p25,p75,color='gray',alpha=0.3)\n",
    "    # Plot the median (50th percentile) as a solid line\n",
    "    ax.plot(p50,lat,color='k',linewidth=1.2)\n",
    "    ax.axvline(0,0,1,color='k',linewidth=1,linestyle='--')\n",
    "    ax.yaxis.tick_right()\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xlim([-2,1])\n",
    "    ax.set_yticks([20,30,40,50],['20N','30N','40N','50N'])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_bars(fig,pos,mean,std,No,title):\n",
    "    hi1,hi2=pos[3]*1/10,pos[3]*9/10\n",
    "    pos1 = [pos[0], pos[1] + hi1+0.008, pos[2], hi2-0.008]  # Upper part\n",
    "    pos2 = [pos[0], pos[1],  pos[2], hi1]\n",
    "    # Create two axes for the broken y-axis\n",
    "    ax1, ax2 = fig.add_axes(pos1), fig.add_axes(pos2)\n",
    "    ax1.set_ylim([-0.5, 0.1])  # Upper part\n",
    "    ax2.set_ylim([-2.42, -2.32])  # Lower part\n",
    "    ax1.axhline(0, 0, 10, color='k',linestyle='--',linewidth=0.8)\n",
    "    colors = ['gray','#FFA726','#33A02C','#1E88E5']\n",
    "    \n",
    "    err_attri = dict(elinewidth=.8, ecolor='k', capsize=4)\n",
    "    ax1.bar(range(1,len(mean)+1),mean,yerr=std,color=colors,error_kw=err_attri)\n",
    "    ax2.bar(range(1,len(mean)+1),mean,yerr=std,color=colors,error_kw=err_attri)\n",
    "    \n",
    "    ax1.set_ylabel('')\n",
    "    ax2.set_yticks([-2.4],[-2.4])\n",
    "    ax1.set_xticks([])\n",
    "    ax2.set_xticks(range(1,len(mean)+1),['China','Arid','Semi','Humid'])\n",
    "    ax1.text(-0.22,1.01,No,transform=ax1.transAxes,fontsize=14,weight='bold')\n",
    "    \n",
    "    # **Draw diagonal break marks**\n",
    "    d = .015  # Offset for diagonal lines\n",
    "    kwargs = dict(transform=ax1.transAxes, color='k', linewidth=1, clip_on=False)\n",
    "    ax1.plot((-d, +d), (-d, +d - 0.015), **kwargs)  # Top-left diagonal\n",
    "    kwargs.update(transform=ax2.transAxes)  # Switch to the lower axes\n",
    "    ax2.plot((-d, +d), (1 - d, 1 + d+0.07), **kwargs)  # Bottom-left diagonal\n",
    "    \n",
    "    ax1.spines[\"bottom\"].set_visible(False)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax2.spines[\"top\"].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax1.set_ylabel('$\\Delta$LST [K]')\n",
    "    ax1.set_title(title)\n",
    "\n",
    "\n",
    "def plot_heatmap(fig, pos, df_slope,cmap,norm,No):\n",
    "    ax = fig.add_axes(pos)\n",
    "    # Convert df_slope to numpy for heatmap plotting\n",
    "    slope_matrix = df_slope.T  # Transpose to match month on x-axis\n",
    "    xticklabels = ['J','F','M','A','M','J','J','A','S','O','N','D']\n",
    "    sns.heatmap(slope_matrix, fmt='', cmap=cmap, norm = norm,cbar=False,\n",
    "                linewidths=0.5, cbar_kws={'label': 'Trend Slope'},#annot_kws={'va':'center'},\n",
    "                xticklabels=xticklabels, yticklabels=slope_matrix.index, ax=ax)\n",
    "    \n",
    "    ax.tick_params(axis='x',length=0)\n",
    "    ax.tick_params(axis='y',length=0)\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('')\n",
    "    ax.text(-0.22,1.05,No,fontsize=14,weight='bold',transform=ax.transAxes)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "# Function to add color bar for the main maps\n",
    "def add_colorbar_map1(ax, mycmap, mynorm, levels,ticklabels):\n",
    "    cb1 = mpl.colorbar.ColorbarBase(ax=ax, cmap=mycmap, norm=mynorm,\n",
    "                                    orientation='vertical',\n",
    "                                    ticks=levels,extend='neither')  # cmap=plt.get_cmap('hot')\n",
    "    ax.set_yticklabels(ticklabels)\n",
    "    ax.set_ylabel('$\\Delta$LST [K]',labelpad=5)\n",
    "    ax.tick_params(axis='y',right=True,length=0)\n",
    "    \n",
    "    \n",
    "# Function to add color bar for the main maps\n",
    "def add_colorbar_map2(ax, mycmap, mynorm):\n",
    "    cb1 = mpl.colorbar.ColorbarBase(ax=ax, cmap=mycmap, norm=mynorm,\n",
    "                                    orientation='vertical',extend='neither')  # cmap=plt.get_cmap('hot')\n",
    "    ax.set_yticks([-2,-1,0,1,2],['<-2','-1','0','1','>2'])\n",
    "    # ax.set_yticklabels([f'{float(tick*10):g}' for tick in ticks])\n",
    "    ax.set_ylabel('$\\Delta$LST [K]',labelpad=5)\n",
    "    ax.tick_params(axis='y',right=True,length=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors1 = plt.cm.BrBG(np.linspace(0.00, 0.40, 128))  # 128* seismic bwr PRGn\n",
    "colors2 = plt.cm.BrBG(np.linspace(0.60, 1.00, 128))  # 128* seismic\n",
    "colors = np.vstack((colors1, colors2))\n",
    "mycmap = mcolors.LinearSegmentedColormap.from_list('my_colormap', colors)\n",
    "plevel = np.array([0,0.1,0.2,0.4,0.8,1.2,1.6,2.0,4.0,10])\n",
    "nlevel = -1 * plevel[1:]\n",
    "levels1 = np.concatenate([nlevel[::-1], plevel])\n",
    "cmap1, norm1 = uneven_cmap(levels1, cmap=mycmap)#RdYlBu_r 'BrBG'\n",
    "\n",
    "Mn_all = np.append(df_dLSTd_Mnmean.iloc[:,1:].values.flatten() ,\n",
    "                   df_dLSTn_Mnmean.iloc[:,1:].values.flatten())\n",
    "vmax1 = Mn_all.max()\n",
    "vmin1 = Mn_all.min()\n",
    "print('vmax, vmin',vmax1,vmin1)\n",
    "k = abs(vmin1) / abs(vmax1)\n",
    "k1_1, k2_1 = int(k * 1000), 1000 \n",
    "print(k,k1_1, k2_1)\n",
    "norm2 = Normalize(vmin = -2, vmax = 2)\n",
    "cmap2 = mcolors.LinearSegmentedColormap.from_list('my_colormap', colors)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plot_settings()\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "\n",
    "x0,y0 = 0.01,0.26\n",
    "xi,yi = 0.145,0.05\n",
    "hx1,vx1 = 0.32,0.5\n",
    "\n",
    "pos3 = [x0+(hx1+xi)*0, y0+(vx1+yi)*0, hx1, vx1]\n",
    "pos4 = [x0+(hx1+xi)*1, y0+(vx1+yi)*0, hx1, vx1]\n",
    "\n",
    "extent = [80, 127,15,54]\n",
    "\n",
    "ax3 = plot_mean_map(fig, pos3, dLSTday_Yr_05d_mean, extent,shp_cn,shp_nanhai,levels1, cmap1, 'c',False,True)\n",
    "ax4 = plot_mean_map(fig, pos4, dLSTnight_Yr_05d_mean, extent,shp_cn,shp_nanhai,levels1, cmap1, 'd',False,True)\n",
    "\n",
    "print(ax3.get_position().width, ax3.get_position().height)\n",
    "\n",
    "# Plot the latitudinal statistics on the right\n",
    "pos1_r = [ax3.get_position().x1+0.01,ax3.get_position().y0,0.07,ax3.get_position().height]\n",
    "pos2_r = [ax4.get_position().x1+0.01,ax4.get_position().y0,0.07,ax4.get_position().height]\n",
    "\n",
    "ax3_r = plot_lat_stats(fig,pos1_r,dLSTday_Yr_05d_mean.Ts,[extent[2],extent[3]])\n",
    "ax4_r = plot_lat_stats(fig,pos2_r,dLSTnight_Yr_05d_mean.Ts,[extent[2],extent[3]])\n",
    "\n",
    "pos1 = [ax3.get_position().x0+0.07,ax3.get_position().y1+0.07,ax3.get_position().width,0.2]\n",
    "pos2 = [ax4.get_position().x0+0.09,ax4.get_position().y1+0.07,ax4.get_position().width,0.2]\n",
    "\n",
    "plot_bars(fig,pos1,df_dLSTd_Yrmean['mean'],df_dLSTd_Yrmean['95p'],'a','Day')\n",
    "plot_bars(fig,pos2,df_dLSTn_Yrmean['mean'],df_dLSTd_Yrmean['95p'],'b','Night')\n",
    "\n",
    "pos5 = [ax3.get_position().x0+0.07,0.07,ax3.get_position().width,0.2]\n",
    "pos6 = [ax4.get_position().x0+0.09,0.07,ax4.get_position().width,0.2]\n",
    "\n",
    "pos_cbar1 = [ax4_r.get_position().x1+0.05,ax4_r.get_position().y0,\n",
    "            0.01,ax4_r.get_position().height]\n",
    "\n",
    "ax_cb1 = fig.add_axes(pos_cbar1)\n",
    "ticklabels1 = ['','-4.0','','-1.6','','-0.8','','-0.2','',\n",
    "    '0','','0.2','','0.8','','1.6','','4.0','']\n",
    "add_colorbar_map1(ax_cb1, cmap1, norm1, levels1, ticklabels1)\n",
    "\n",
    "ax5 = plot_heatmap(fig, pos5, df_dLSTd_Mnmean.iloc[:,1:],cmap2,norm2,'e')\n",
    "ax6 = plot_heatmap(fig, pos6, df_dLSTn_Mnmean.iloc[:,1:],cmap2,norm2,'f')\n",
    "\n",
    "pos_cbar2 = [ax6.get_position().x1+0.03,ax6.get_position().y0,\n",
    "            0.01,ax6.get_position().height]\n",
    "ax_cb2 = fig.add_axes(pos_cbar2)\n",
    "\n",
    "add_colorbar_map2(ax_cb2, cmap2, norm2)\n",
    "\n",
    "plt.savefig('../figures/Figure_S02.png',dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
